---
title: "Marketing Assignment"
author: "Felix Mueller"
date: "2/6/2017"
output: 
  html_document:
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r PREP}
require(readr)
require(dplyr)
require(ggplot2)
require(gridExtra)
require(pROC)
require(caret)
require(e1071)
require(randomForest)
require(ROSE)
require(FactoMineR)
setwd("~/Google Drive/IE/Marketing Intelligence/IE_MI_IndividualProject_data")
```


```{r prepare & read the data, include=FALSE}
IE_MI_training <- read_delim("~/Google Drive/IE/Marketing Intelligence/IE_MI_IndividualProject_data/IE_MI_training.txt",";", escape_double = FALSE, trim_ws = TRUE)
IE_MI_training$prov <- as.integer(IE_MI_training$prov)
df <- IE_MI_training %>% select(-uuid) 
df <- ovun.sample(target~.,data=df,method="under", p=0.1)$data
```

```{r}
names <- c('device' ,'os','os_version','browser','prov','day','ind_hour_0','ind_hour_1','ind_hour_2','ind_hour_3','ind_hour_4','ind_hour_5','ind_hour_6','ind_hour_7','ind_hour_8','ind_hour_9','ind_hour_10','ind_hour_11','ind_hour_12','ind_hour_13','ind_hour_14','ind_hour_15','ind_hour_16','ind_hour_17','ind_hour_18','ind_hour_19','ind_hour_20','ind_hour_21','ind_hour_22','ind_hour_23','target')
df[,names] <- lapply(df[,names], factor)

cols = sapply(df,function(x){is.integer(x)|is.numeric(x)})

#only using correlation is useless because the values of cor are REALLY small (<0.05)
# apply PCA - scale. = TRUE is highly 
# advisable, but default is FALSE. 
pca <- prcomp(df[,cols],center = TRUE,scale. = TRUE) 
# summary(pca) # tells us to use only 8 PCs
numbers = data.frame(pca$x[,1:8])

#removes PC8
cor_matrix = cor(numbers,as.numeric(df$target),method="spearman")
names <- rownames(subset(cor_matrix,abs(cor_matrix) >= 0.05))
numbers <- numbers %>% select(one_of(names))

cols = sapply(df,function(x){is.factor(x)})
factors <- df[,cols] %>% select(-target)
factors_non_daytime <- df %>% select(device,os,os_version,browser,prov,day)

#for the daytime
daytime <- factors %>% select(-device,-os,-os_version,-browser,-prov,-day)
mca <- MCA(daytime)
summary(mca)
daytime_mca_vals <- mca$ind$coord

target <- df$target
filtered_df <- cbind(daytime_mca_vals,factors_non_daytime,numbers,target)
dim(filtered_df)
write.csv2(file="filtered_data.csv",x=filtered_df)
```


```{r define the function that splits the dataframe}
splitdf <- function(dataframe, seed=1, percentage=0.8) {
  if (!is.null(seed)) set.seed(seed)
  index <- 1:nrow(dataframe)
  numTrainingSamples <- round(length(index) * percentage)
  trainindex <- sample(index, numTrainingSamples)
  trainset <- dataframe[trainindex, ]
  testset <- dataframe[-trainindex, ]
  list(trainset=trainset,testset=testset)
}
```

# Defining the MCC metric
I will compare the MCC with the ROC curve. For that reason I used the MCC function you provided in class.
```{r Defining the MCC metric}
## function to compute the Matthews Correlation Coefficient (MCC) in a classification framework
## ct: confusion table (matrix or table)
## nbcat: the actual number of categories for the truth and the predictions
## Example:
`mcc` <- function(ct, nbcat=nrow(ct)) {
    if(nrow(ct) != ncol(ct)) { stop("the confusion table should be square!") }
    if(!(sum(ct)==sum(diag(ct))) &&  (length(which(apply(ct, 1, sum) == 0)) == (nbcat-1) & ((length(which(apply(ct, 2, sum) == 0)) != (nbcat-1)) | (length(which(apply(ct, 2, sum) == 0)) == (nbcat-1)))) || (length(which(apply(ct, 2, sum) == 0)) == (nbcat-1) & ((length(which(apply(ct, 1, sum) == 0)) != (nbcat-1)) | (length(which(apply(ct, 1, sum) == 0)) == (nbcat-1)) & sum(diag(ct)) == 0))) { ct <- ct + matrix(1, ncol=nbcat, nrow=nbcat) } ### add element to categories if nbcat-1 predictive categories do not contain elements. Not in case where all are correct!
    
    if(sum(ct, na.rm=TRUE) <= 0) { return(NA) }
    
    myx <- matrix(TRUE, nrow=nrow(ct), ncol=ncol(ct))
    diag(myx) <- FALSE
    if(sum(ct[myx]) == 0) { return(1) }
    myperf <- 0
    for(k in 1:nbcat) {
      for(m in 1:nbcat) {
        for(l in 1:nbcat) {
          myperf <- myperf + ((ct[k, k] * ct[m, l]) - (ct[l, k] * ct[k, m]))
        }
      }
    }
    aa <- 0
    for(k in 1:nbcat) {
      cc <- 0
      for(l in 1:nbcat) { cc <- cc + ct[l, k] }
      dd <- 0
      for(f in 1:nbcat) {
        for(g in 1:nbcat) { if(f != k) { dd <- dd + ct[g, f] } }
      }
      aa <- aa + (cc * dd)
    }
    bb <- 0
    for(k in 1:nbcat) {
      cc <- 0
      for(l in 1:nbcat) { cc <- cc + ct[k, l] }
      dd <- 0
      for(f in 1:nbcat) {
        for(g in 1:nbcat) { if(f != k) { dd <- dd + ct[f, g] } }
      }
      bb <- bb + (cc * dd)
    }
    
    myperf <- myperf / (sqrt(aa) * sqrt(bb))
    return(myperf)
  }
```

```{r}
getThreshold <- function(predictions){
  myROC <- roc(target ~ pred, predictions)
  optimalThreshold <- coords(myROC, "best", ret = "threshold")
}

logReg <- function(train,cv,test,seed=1){
  set.seed(seed)
  # The logistic regression over most of the features, and the training data set.
  model <- glm(target~., data=train, family = "binomial")
  #optimize the model with backwards stepwise feature selection
  model <- step(model,direction="backward",trace=F,steps=10)  
  probs <- predict(model, type="response", newdata = cv)
  predictions <- data.frame(target=cv$target, pred=probs)
  threshold <- getThreshold(predictions)
  #now we predict for our test set
  probs <- predict(model, type="response", newdata = test)
  predictions <- data.frame(target=test$target, pred=probs)
  #diffusion Table
  tbl <- table(predictions$target, predictions$pred > threshold)
  #print(coef(model))
  auc = auc(predictions$target, predictions$pred)
  return(list(tbl=tbl,auc=auc,threshold=threshold))
}

randf <- function(train,cv,test,seed=1){
  set.seed(seed)
  model <- randomForest(target~., data=train)
  #rf <- rfe(cv, vtarget,c(5:15),rfeControl = rfeControl(functions = rfFuncs, method = "cv"))
  #train = trn[rf$optVariables]
  probs <- predict(model, type="prob", newdata = cv)  
  predictions <- data.frame(target=cv$target, pred=probs[,2])
  threshold <- getThreshold(predictions)
  #now we predict for our test set
  #,threshold=threshold
  probs <- predict(model, type="prob", newdata = test)
  predictions <- data.frame(target=test$target, pred=probs[,2])
  #diffusion Table
  tbl <- table(predictions$target, predictions$pred > threshold)
  auc = auc(predictions$target, predictions$pred)
  return(list(tbl=tbl,auc=auc,threshold=threshold))
}

supVM <- function(train,cv,test,seed=1){
  set.seed(seed)
  model <- svm(target~., data=train,probability=T)
  probs <- predict(model, type="prob", newdata = cv,probability=T)  
  predictions <- data.frame(target=cv$target, pred=attr(probs, "probabilities")[,2])
  threshold <- getThreshold(predictions)
  #now we predict for our test set
  probs <- predict(model, type="prob", newdata = test,probability=T)
  predictions <- data.frame(target=test$target, pred=attr(probs, "probabilities")[,2])
  #diffusion Table
  tbl <- table(predictions$target, predictions$pred > threshold)
  auc = auc(predictions$target, predictions$pred)
  return(list(tbl=tbl,auc=auc,threshold=threshold))
}
```

# Defining the function for one complete model creation

```{r define the function for one complete model creation}
create_model <- function(seed=1,applyRF=F,applySVM=F){
  splits <- splitdf(dataframe=df, percentage = 0.6,seed = seed)
  trn = splits$trainset
  splits <- splitdf(splits$testset, percentage = 0.5,seed = seed)
  cv = splits$trainset
  tst = splits$testset
  if(applyRF==T){
    li = randf(trn,cv,tst,seed=seed)
    tbl = li$tbl
  }
  else if(applySVM==T){
    li = supVM(trn,cv,tst,seed=seed)
    tbl = li$tbl
  }
  else{
    li = logReg(trn,cv,tst,seed=seed)
    tbl = li$tbl
  }
  print(tbl)
  #let's calculate the evaluation measures
  accuracy <- (tbl[1,1] + tbl[2,2]) / sum(tbl)
  F1 <- (2*(tbl[1,1]))/((2*(tbl[1,1]))+tbl[2,1]+tbl[1,2])
  auc = li$auc
  threshold = li$threshold
  return(list(accuracy,F1,auc,threshold))
}
```

# Iteration over the models
Now I iterated over the functions created before in order to obtain several models and their performance metrics for comparison.
```{r Iteration over the models, message = FALSE,warning=FALSE}
df = read.csv2("filtered_data.csv") %>% select(-X)
df <- df %>% select(one_of(c("Dim.1","Dim.2","os","PC3","PC4","PC5","PC6","PC7","target")))
df$target <- as.factor(df$target)
accuracy <- vector()
f1_measure <- vector()
coefis <- list()
optimalThreshold <- vector()
auc = vector()

sequence <- seq(1,10)
for (i in sequence){
  tmp = create_model(seed=i,applySVM=F,applyRF = T)
  accuracy[i] <- tmp[[1]]
  f1_measure[i] <- tmp[[2]]
  auc[i] <- tmp[[3]]
  optimalThreshold[i] <- tmp[[4]]
  print(i)
}
```

The MCC is more useful for unbalanced datasets. Since the dataset is more or less unbalanced (Proportion = 3:2) it might a more useful measure than Accuracy. That's why I chose it to compare with ROC. 
I ran a for loop that iterates 100 times and retrieves the accuracy, F1 metric, coefficients and the optimal threshold.

# Comparison of all models regarding F1_measure and Accuracy
Okay, now let's have a look at the results:
```{r compare all models regarding F1_measure and Accuracy, collapse = T}
q1 = qplot(y=accuracy,x=sequence,xlab="Iteration",ylab="Accuracy",main="Accuracy per iteration")+
  geom_line(colour=I("blue"))+
  geom_point(aes(x = which(accuracy==max(accuracy)), y = max(accuracy)), color = I("red"))+
  geom_hline(aes(yintercept=max(accuracy)),color = I("red"),linetype = 2)
q2 = qplot(y=f1_measure,x=sequence,xlab="Iteration",ylab="F1 Measure",main="F1 Measure per iteration")+
  geom_line(colour=I("blue"))+
  geom_point(aes(x = which(f1_measure==max(f1_measure)), y = max(f1_measure)), color = I("red"))+
  geom_hline(aes(yintercept=max(f1_measure)),color = I("red"),linetype = 2)
q3 = qplot(y=auc,x=sequence,xlab="Iteration",ylab="AUC",main="AUC per iteration")+
  geom_line(colour=I("blue"))+
  geom_point(aes(x = which(auc==max(auc)), y = max(auc)), color = I("red"))+
  geom_hline(aes(yintercept=max(auc)),color = I("red"),linetype = 2)
grid.arrange(q1,q2,q3)
cat(paste("Highest Accuracy:", max(accuracy), ", Lowest Accuracy:", min(accuracy),", Average Accuracy:", mean(accuracy)))
cat(paste("Highest F1:", max(f1_measure),", Lowest F1:", min(f1_measure),", Average F1:", mean(f1_measure)))
cat(paste("Highest AUC:", max(auc), ", Lowest AUC:", min(auc),", Average AUC:", mean(auc)))
cat(paste("Is the performance the highest for all three measures?:", which(accuracy == max(accuracy))==which(f1_measure == max(f1_measure))&which(auc == max(auc))==which(accuracy == max(accuracy))))
cat(paste("The seed I should set to obtain the optimal model:", which(accuracy == max(accuracy))))

#SVM seed = 1:10
#Highest Accuracy: 0.86545561955398 , Lowest Accuracy: 0.18150937823069 , Average Accuracy: 0.653300841825432
#Highest F1: 0.927102504601104 , Lowest F1: 0.202360391479562 , Average F1: 0.744306569550274
#SVM seed = 11:20 is not better
#Random Forest has much better AUC --> ~0.65
# LogReg even better: --> ~0.68; also, very constant for all measures
```
Interestingly, the obtained F1 scores and accuracies are very similar for the values optimized by the ROC and by the MCC. This might be due to a quite balanced dataset with proportions of 3:2. However, the models achieve slightly better scores on average. which is mainly due to less negative outliers.

As we can see on the two plots the performance varies greatly between 0.86 and 0.7 for both F1 and Accuracy The final model I select is the model obtained with seed = 92 which is optimal for both metrics. In addition, I choose the model obtained by the MCC tuning because it obtained slightly better scores.

```{r}
new_dat <- read_delim("~/Google Drive/IE/Marketing Intelligence/IE_MI_IndividualProject_data/IE_MI_validation.txt",";", escape_double = FALSE, trim_ws = TRUE)
#IE_MI_validation
pca_new = predict(pca, newdata=new_dat)
factors_new <-new_dat %>% select(one_of(names(factors)))
target <- df$target
new_data <- cbind(factors,pca_new,target)
```

```{r}
df = read.csv2("filtered_data.csv") %>% select(-X)
df$target <- as.factor(df$target)
accuracy <- vector()
f1_measure <- vector()
coefis <- list()
optimalThreshold <- vector()

create_model <- function(seed=1,applyRF=F,applySVM=F){
  set.seed(seed)
  splits <- splitdf(dataframe=df, percentage = 0.8,seed = seed)
  trn = splits$trainset
  tst = splits$testset
  best_model <- best.svm(target~., data = trn, gamma = 2^(-1:1), cost = 2^(2:4),probability=T)

  probs <- predict(model, type="prob", newdata = cv,probability=T)  
  predictions <- data.frame(target=cv$target, pred=attr(probs, "probabilities")[,2])
  threshold <- getThreshold(predictions)
  #now we predict for our test set
  probs <- predict(model, type="prob", newdata = test,probability=T)
  predictions <- data.frame(target=test$target, pred=attr(probs, "probabilities")[,2])
  #diffusion Table
  tbl <- table(predictions$target, predictions$pred > threshold)
  
  print(table(cv$target))
  print(table(tst$target))
  print(tbl)
  #let's calculate the evaluation measures
  accuracy <- (tbl[1,1] + tbl[2,2]) / sum(tbl)
  F1 <- (2*(tbl[1,1]))/((2*(tbl[1,1]))+tbl[2,1]+tbl[1,2])
  return(list(accuracy,F1))
}


```




